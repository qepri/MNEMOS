# LLM Configuration
# Options: openai, anthropic, lm_studio, ollama
LLM_PROVIDER=lm_studio

# OpenAI
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Anthropic
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# Local LLM (LM Studio / Ollama)
LOCAL_LLM_BASE_URL=http://host.docker.internal:1234/v1
LOCAL_LLM_MODEL=local-model

# Embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384

# Embedding Optimization (New - Performance Tuning)
# EMBEDDING_BATCH_SIZE: 0 = auto-detect based on GPU VRAM, or set manually (e.g., 32, 64, 128)
EMBEDDING_BATCH_SIZE=0
# EMBEDDING_DEVICE: auto, cpu, cuda, mps (Apple Silicon)
EMBEDDING_DEVICE=auto
# EMBEDDING_USE_FP16: Use mixed precision on GPU (2x faster, half VRAM usage)
EMBEDDING_USE_FP16=true
# EMBEDDING_SHOW_PROGRESS: Show progress bar for embedding generation
EMBEDDING_SHOW_PROGRESS=true

# Remote API Optimization (OpenAI, LM Studio, Ollama)
# REMOTE_EMBEDDING_BATCH_SIZE: How many texts to send per API call
REMOTE_EMBEDDING_BATCH_SIZE=32
# REMOTE_EMBEDDING_MAX_WORKERS: Parallel API requests (careful with rate limits)
REMOTE_EMBEDDING_MAX_WORKERS=3
# REMOTE_EMBEDDING_RETRY_DELAY: Seconds to wait before retrying failed requests
REMOTE_EMBEDDING_RETRY_DELAY=2.0

# Whisper
WHISPER_MODEL=base
WHISPER_DEVICE=cpu

# Secret Key
SECRET_KEY=dev-secret-key-change-me
