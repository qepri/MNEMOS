services:
  frontend:
    build: ./frontend_spa
    ports:
      - "5200:80"
    depends_on:
      - app

  app:
    build: .
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=development
      - DATABASE_URL=postgresql://raguser:ragpass@db:5432/ragdb
      - REDIS_URL=redis://redis:6379/0
      # Ensure python doesn't buffer output so we see logs in realtime
      - PYTHONUNBUFFERED=1
      - OLLAMA_NUM_CTX=2048
      - EMBEDDING_PROVIDER=local
      - EMBEDDING_DEVICE=cpu
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - EMBEDDING_DIMENSION=384
      - HOST_PROJECT_PATH=${HOST_PROJECT_PATH:-.}
    volumes:
      - ./app:/app/app:rw
      - ./config:/app/config:rw
      - ./.env:/app/.env:ro
      - ./ollama_models/import:/app/ollama_import:ro
      - uploads_data:/app/uploads
      - whisper_cache:/root/.cache/whisper
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    extra_hosts:
      - "host.docker.internal:host-gateway" # For LM Studio access

  worker:
    build: .
    command: celery -A app.celery_app worker --loglevel=info
    environment:
      - DATABASE_URL=postgresql://raguser:ragpass@db:5432/ragdb
      - REDIS_URL=redis://redis:6379/0
      - PYTHONUNBUFFERED=1
      - OLLAMA_NUM_CTX=2048
      - EMBEDDING_PROVIDER=local
      - EMBEDDING_DEVICE=cpu
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - EMBEDDING_DIMENSION=384
      - HOST_PROJECT_PATH=${HOST_PROJECT_PATH:-.}
    volumes:
      - ./app:/app/app:rw
      - ./config:/app/config:rw
      - ./.env:/app/.env:ro
      - uploads_data:/app/uploads
      - whisper_cache:/root/.cache/whisper
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    extra_hosts:
      - "host.docker.internal:host-gateway"

  ollama:
    image: ollama/ollama:latest
    container_name: mnemos-ollama
    profiles: [ "manual" ]
    ports:
      - "11435:11434"
    volumes:
      - ./ollama_models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    environment:
      - OLLAMA_FLASH_ATTENTION=0
      - OLLAMA_GPU_LAYERS=-1
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_MAX_LOADED_MODELS=1
    restart: no

  mcp:
    build: .
    command: python -m app.mcp_server.server
    ports:
      - "3000:3000"
    environment:
      - DATABASE_URL=postgresql://raguser:ragpass@db:5432/ragdb
      - PYTHONUNBUFFERED=1
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - ./app:/app/app:rw
      - ./config:/app/config:rw

  db:
    image: pgvector/pgvector:pg16
    # Optimization: Tune Postgres for Vector Search
    # shared_buffers: ~25% of RAM (128MB is conservative default, setting 512MB)
    # work_mem: Memory for complex sorts/queries (vector search needs this)
    # maintenance_work_mem: Speed up index creation (HNSW build)
    command: >
      postgres  -c shared_buffers=512MB  -c work_mem=32MB  -c maintenance_work_mem=256MB
    environment:
      - POSTGRES_USER=raguser
      - POSTGRES_PASSWORD=ragpass
      - POSTGRES_DB=ragdb
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U raguser -d ragdb" ]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    # Optimization: Enable AOF (Append Only File) persistence
    # This prevents data loss (like Celery tasks) if the container restarts.
    command: [ "redis-server", "--appendonly", "yes" ]
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  uploads_data:
  whisper_cache:
  redis_data:
